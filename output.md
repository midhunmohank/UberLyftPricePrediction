# Lyft and Uber cab fare prediction
-----------------------------------------------------------------

In this project, we are predicting the Lyft's and Uber's cab fares for future rides. We want to determine the price of the ride not just by distance but also by the weather conditions, time of day, and day if the week. 

Uber and Lyft, fortunately have a rich data set since there have been thousands of customers since both companies have started. This also means managing their data properly, enabling one to explore data to identify revelant data points, extract meaningful insights so that we can solve business problems such as this.

The goal of this predictive model is to generate a fare that improves customer trust and satisfaction without comprimising on comfort of the driver. Customer trust is the underlying goal since a trust-worthy company is more likely to be chosen than the competitors.

## Problem statement
____________________________________________________

The Uber/Lyft cab fare that is generated by the system takes only distance into consideration. However, the following problems are common while booking a cab:

1. Bad weather conditions
2. Bad Traffic conditions 
3. Cab is booked at an odd time (Ex. After mid night)
4. High demand of cab due to weekend or during peak hours

Since these factors are not taken into consideration, as a result:

1. Going to the pick up point can be an arduous process for the driver
2. Covering short distances can be tough
3. The cab driver needs to comprimise on sleep for duty
4. The customer is frustrated if the cab cancels in the last minute
5. Gas usage and expenditure are high

The customer is dissatisfied, the driver has extra expenditure and low income, and Uber's business is comprimised. 

#### This notebook explores various Machine Learning models that considers the above mentioned factors to predict a suitable fare.

## Plan of action:
____________________________________________________

STEP 1. Collecting the Raw Data and Data pre-processing
  
STEP 2. Exploratory Data Analysis - 
    
    EDA refers to the critical process of performing initial investigations on data to discover patterns, spot anomalies, test hypothesis, and to check assumptions with the help of summary statistics and graphical representations. Understand the data is powerful to gather insights from it. In short, EDA is all about making sense of data in hand.
    
STEP 3. Data Preparation -
    
    1. Label Encoding - 
    
    Label Encoding refers to converting the labels into a numeric form so as to convert them into the machine-readable form.
    
    2. Binning -
    
    Binning is the process of transforming numerical variables into categorical counterparts. Binning improves accuracy of the predictive models by reducing the noise or non-linearity in the dataset. Finally, binning lets easy identification of outliers, invalid and missing values of numerical variables.
    
STEP 4. Recursive Feature Elimination -
    
    RFE is a wrapper-type feature selection algorithm. The Machine Learning algorithm that used in the core of the method, is wrapped by RFE, which is used to help select features. This is in contrast to filter-based feature selections that score each feature and select those features with the largest (or smallest) score. 
    
    RFE is a wrapper-style feature selection algorithm that also uses filter-based feature selection internally.
    
STEP 5. Feature Selection -

    A machine learning dataset for classification or regression is comprised of rows and columns, like an excel spreadsheet. 
     
    Rows are often referred to as samples and columns are referred to as features. Feature engineering is selecting and transforming a subset of the most relevant features (columns) for a dataset.
    
    Fewer features can allow machine learning algorithms to run more efficiently (less space or time complexity) and be more effective. Some machine learning algorithms can be misled by irrelevant input features, resulting in worse predictive performance.

STEP 6. Modelling and Testing-
    
    Training a machine learning algorithm to predict the labels from the features, tuning it for the business need, and validating it on holdout data.
    
    We have explored 4 ML models, namely - 
    1. Linear Regression
    2. Random Forest
    3. Decision Tree
    4. Gradient Boosting Regressor
    
    Modelling:
    1. Fitting
    2. Accuracy score
    3. Hyper-parameter tuning
    
    Evaluation:
    
    1. Actual vs Predicted
    2. Cross-validation
    3. Performance Metric of each algorithm
    
STEP 7. Results

### Packages used

Pandas - to represent the raw data and give it some structure

Numpy - to perform all mathematical and scientific computations on the operations

Sklearn - to select efficient tools for the ML models like regression, fitting and predicting  

Matplotlib and seaborn - for data visualization and plotting graphs


### STEP 1 Collecting the raw data and data pre-processing
___________________________________


```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE
```


```python
uber_dataset = pd.read_csv(r'rideshare_kaggle.csv')
print(uber_dataset.shape)
```

    (693071, 57)



```python
uber_dataset.shape
```




    (693071, 57)



The data is the the form of an Excel spreadsheet and has 693071 rows and 57 columns. We are representing the data in the form of a dataframe called uber_dataset


```python
uber_dataset.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>month</th>
      <th>datetime</th>
      <th>timezone</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>...</th>
      <th>precipIntensityMax</th>
      <th>uvIndexTime</th>
      <th>temperatureMin</th>
      <th>temperatureMinTime</th>
      <th>temperatureMax</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMin</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMax</th>
      <th>apparentTemperatureMaxTime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>
      <td>1.544953e+09</td>
      <td>9</td>
      <td>16</td>
      <td>12</td>
      <td>2018-12-16 09:30:07</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>0.1276</td>
      <td>1544979600</td>
      <td>39.89</td>
      <td>1545012000</td>
      <td>43.68</td>
      <td>1544968800</td>
      <td>33.73</td>
      <td>1545012000</td>
      <td>38.07</td>
      <td>1544958000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>
      <td>1.543284e+09</td>
      <td>2</td>
      <td>27</td>
      <td>11</td>
      <td>2018-11-27 02:00:23</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>0.1300</td>
      <td>1543251600</td>
      <td>40.49</td>
      <td>1543233600</td>
      <td>47.30</td>
      <td>1543251600</td>
      <td>36.20</td>
      <td>1543291200</td>
      <td>43.92</td>
      <td>1543251600</td>
    </tr>
    <tr>
      <th>2</th>
      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>
      <td>1.543367e+09</td>
      <td>1</td>
      <td>28</td>
      <td>11</td>
      <td>2018-11-28 01:00:22</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>0.1064</td>
      <td>1543338000</td>
      <td>35.36</td>
      <td>1543377600</td>
      <td>47.55</td>
      <td>1543320000</td>
      <td>31.04</td>
      <td>1543377600</td>
      <td>44.12</td>
      <td>1543320000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>
      <td>1.543554e+09</td>
      <td>4</td>
      <td>30</td>
      <td>11</td>
      <td>2018-11-30 04:53:02</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>0.0000</td>
      <td>1543507200</td>
      <td>34.67</td>
      <td>1543550400</td>
      <td>45.03</td>
      <td>1543510800</td>
      <td>30.30</td>
      <td>1543550400</td>
      <td>38.53</td>
      <td>1543510800</td>
    </tr>
    <tr>
      <th>4</th>
      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>
      <td>1.543463e+09</td>
      <td>3</td>
      <td>29</td>
      <td>11</td>
      <td>2018-11-29 03:49:20</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>0.0001</td>
      <td>1543420800</td>
      <td>33.10</td>
      <td>1543402800</td>
      <td>42.18</td>
      <td>1543420800</td>
      <td>29.11</td>
      <td>1543392000</td>
      <td>35.75</td>
      <td>1543420800</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 57 columns</p>
</div>



As shown above, the raw data is only structured at this point. There are various datatypes in a single dataset. 
1. The dataset needs be narrowed down to a single datatype
2. The date columns need to be modified
3. The null values need to be handled
4. Special characters need to be handled

In short, the purpose of cleaning the data is to make it more meaningful for the ML model

Shown below is the metadata, and the corresponding data types


```python
uber_dataset.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 693071 entries, 0 to 693070
    Data columns (total 57 columns):
     #   Column                       Non-Null Count   Dtype  
    ---  ------                       --------------   -----  
     0   id                           693071 non-null  object 
     1   timestamp                    693071 non-null  float64
     2   hour                         693071 non-null  int64  
     3   day                          693071 non-null  int64  
     4   month                        693071 non-null  int64  
     5   datetime                     693071 non-null  object 
     6   timezone                     693071 non-null  object 
     7   source                       693071 non-null  object 
     8   destination                  693071 non-null  object 
     9   cab_type                     693071 non-null  object 
     10  product_id                   693071 non-null  object 
     11  name                         693071 non-null  object 
     12  price                        637976 non-null  float64
     13  distance                     693071 non-null  float64
     14  surge_multiplier             693071 non-null  float64
     15  latitude                     693071 non-null  float64
     16  longitude                    693071 non-null  float64
     17  temperature                  693071 non-null  float64
     18  apparentTemperature          693071 non-null  float64
     19  short_summary                693071 non-null  object 
     20  long_summary                 693071 non-null  object 
     21  precipIntensity              693071 non-null  float64
     22  precipProbability            693071 non-null  float64
     23  humidity                     693071 non-null  float64
     24  windSpeed                    693071 non-null  float64
     25  windGust                     693071 non-null  float64
     26  windGustTime                 693071 non-null  int64  
     27  visibility                   693071 non-null  float64
     28  temperatureHigh              693071 non-null  float64
     29  temperatureHighTime          693071 non-null  int64  
     30  temperatureLow               693071 non-null  float64
     31  temperatureLowTime           693071 non-null  int64  
     32  apparentTemperatureHigh      693071 non-null  float64
     33  apparentTemperatureHighTime  693071 non-null  int64  
     34  apparentTemperatureLow       693071 non-null  float64
     35  apparentTemperatureLowTime   693071 non-null  int64  
     36  icon                         693071 non-null  object 
     37  dewPoint                     693071 non-null  float64
     38  pressure                     693071 non-null  float64
     39  windBearing                  693071 non-null  int64  
     40  cloudCover                   693071 non-null  float64
     41  uvIndex                      693071 non-null  int64  
     42  visibility.1                 693071 non-null  float64
     43  ozone                        693071 non-null  float64
     44  sunriseTime                  693071 non-null  int64  
     45  sunsetTime                   693071 non-null  int64  
     46  moonPhase                    693071 non-null  float64
     47  precipIntensityMax           693071 non-null  float64
     48  uvIndexTime                  693071 non-null  int64  
     49  temperatureMin               693071 non-null  float64
     50  temperatureMinTime           693071 non-null  int64  
     51  temperatureMax               693071 non-null  float64
     52  temperatureMaxTime           693071 non-null  int64  
     53  apparentTemperatureMin       693071 non-null  float64
     54  apparentTemperatureMinTime   693071 non-null  int64  
     55  apparentTemperatureMax       693071 non-null  float64
     56  apparentTemperatureMaxTime   693071 non-null  int64  
    dtypes: float64(29), int64(17), object(11)
    memory usage: 301.4+ MB


The describe() function computes a summary of statistics pertaining to the DataFrame columns. It defines the mean, std values.


```python
uber_dataset.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>month</th>
      <th>price</th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>...</th>
      <th>precipIntensityMax</th>
      <th>uvIndexTime</th>
      <th>temperatureMin</th>
      <th>temperatureMinTime</th>
      <th>temperatureMax</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMin</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMax</th>
      <th>apparentTemperatureMaxTime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6.930710e+05</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>637976.000000</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>693071.000000</td>
      <td>...</td>
      <td>693071.000000</td>
      <td>6.930710e+05</td>
      <td>693071.000000</td>
      <td>6.930710e+05</td>
      <td>693071.000000</td>
      <td>6.930710e+05</td>
      <td>693071.000000</td>
      <td>6.930710e+05</td>
      <td>693071.000000</td>
      <td>6.930710e+05</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.544046e+09</td>
      <td>11.619137</td>
      <td>17.794365</td>
      <td>11.586684</td>
      <td>16.545125</td>
      <td>2.189430</td>
      <td>1.013870</td>
      <td>42.338172</td>
      <td>-71.066151</td>
      <td>39.584388</td>
      <td>...</td>
      <td>0.037374</td>
      <td>1.544044e+09</td>
      <td>33.457774</td>
      <td>1.544042e+09</td>
      <td>45.261313</td>
      <td>1.544047e+09</td>
      <td>29.731002</td>
      <td>1.544048e+09</td>
      <td>41.997343</td>
      <td>1.544048e+09</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.891925e+05</td>
      <td>6.948114</td>
      <td>9.982286</td>
      <td>0.492429</td>
      <td>9.324359</td>
      <td>1.138937</td>
      <td>0.091641</td>
      <td>0.047840</td>
      <td>0.020302</td>
      <td>6.726084</td>
      <td>...</td>
      <td>0.055214</td>
      <td>6.912028e+05</td>
      <td>6.467224</td>
      <td>6.901954e+05</td>
      <td>5.645046</td>
      <td>6.901353e+05</td>
      <td>7.110494</td>
      <td>6.871862e+05</td>
      <td>6.936841</td>
      <td>6.910777e+05</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.543204e+09</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>11.000000</td>
      <td>2.500000</td>
      <td>0.020000</td>
      <td>1.000000</td>
      <td>42.214800</td>
      <td>-71.105400</td>
      <td>18.910000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>1.543162e+09</td>
      <td>15.630000</td>
      <td>1.543122e+09</td>
      <td>33.510000</td>
      <td>1.543154e+09</td>
      <td>11.810000</td>
      <td>1.543136e+09</td>
      <td>28.950000</td>
      <td>1.543187e+09</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.543444e+09</td>
      <td>6.000000</td>
      <td>13.000000</td>
      <td>11.000000</td>
      <td>9.000000</td>
      <td>1.280000</td>
      <td>1.000000</td>
      <td>42.350300</td>
      <td>-71.081000</td>
      <td>36.450000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>1.543421e+09</td>
      <td>30.170000</td>
      <td>1.543399e+09</td>
      <td>42.570000</td>
      <td>1.543439e+09</td>
      <td>27.760000</td>
      <td>1.543399e+09</td>
      <td>36.570000</td>
      <td>1.543439e+09</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.543737e+09</td>
      <td>12.000000</td>
      <td>17.000000</td>
      <td>12.000000</td>
      <td>13.500000</td>
      <td>2.160000</td>
      <td>1.000000</td>
      <td>42.351900</td>
      <td>-71.063100</td>
      <td>40.490000</td>
      <td>...</td>
      <td>0.000400</td>
      <td>1.543770e+09</td>
      <td>34.240000</td>
      <td>1.543727e+09</td>
      <td>44.680000</td>
      <td>1.543788e+09</td>
      <td>30.130000</td>
      <td>1.543745e+09</td>
      <td>40.950000</td>
      <td>1.543788e+09</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.544828e+09</td>
      <td>18.000000</td>
      <td>28.000000</td>
      <td>12.000000</td>
      <td>22.500000</td>
      <td>2.920000</td>
      <td>1.000000</td>
      <td>42.364700</td>
      <td>-71.054200</td>
      <td>43.580000</td>
      <td>...</td>
      <td>0.091600</td>
      <td>1.544807e+09</td>
      <td>38.880000</td>
      <td>1.544789e+09</td>
      <td>46.910000</td>
      <td>1.544814e+09</td>
      <td>35.710000</td>
      <td>1.544789e+09</td>
      <td>44.120000</td>
      <td>1.544818e+09</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.545161e+09</td>
      <td>23.000000</td>
      <td>30.000000</td>
      <td>12.000000</td>
      <td>97.500000</td>
      <td>7.860000</td>
      <td>3.000000</td>
      <td>42.366100</td>
      <td>-71.033000</td>
      <td>57.220000</td>
      <td>...</td>
      <td>0.145900</td>
      <td>1.545152e+09</td>
      <td>43.100000</td>
      <td>1.545192e+09</td>
      <td>57.870000</td>
      <td>1.545109e+09</td>
      <td>40.050000</td>
      <td>1.545134e+09</td>
      <td>57.200000</td>
      <td>1.545109e+09</td>
    </tr>
  </tbody>
</table>
<p>8 rows Ã— 46 columns</p>
</div>



Price has 55095 null values that need to be handled.


```python
#Finding the null Values in In column
uber_dataset.isnull().sum()
```




    id                                 0
    timestamp                          0
    hour                               0
    day                                0
    month                              0
    datetime                           0
    timezone                           0
    source                             0
    destination                        0
    cab_type                           0
    product_id                         0
    name                               0
    price                          55095
    distance                           0
    surge_multiplier                   0
    latitude                           0
    longitude                          0
    temperature                        0
    apparentTemperature                0
    short_summary                      0
    long_summary                       0
    precipIntensity                    0
    precipProbability                  0
    humidity                           0
    windSpeed                          0
    windGust                           0
    windGustTime                       0
    visibility                         0
    temperatureHigh                    0
    temperatureHighTime                0
    temperatureLow                     0
    temperatureLowTime                 0
    apparentTemperatureHigh            0
    apparentTemperatureHighTime        0
    apparentTemperatureLow             0
    apparentTemperatureLowTime         0
    icon                               0
    dewPoint                           0
    pressure                           0
    windBearing                        0
    cloudCover                         0
    uvIndex                            0
    visibility.1                       0
    ozone                              0
    sunriseTime                        0
    sunsetTime                         0
    moonPhase                          0
    precipIntensityMax                 0
    uvIndexTime                        0
    temperatureMin                     0
    temperatureMinTime                 0
    temperatureMax                     0
    temperatureMaxTime                 0
    apparentTemperatureMin             0
    apparentTemperatureMinTime         0
    apparentTemperatureMax             0
    apparentTemperatureMaxTime         0
    dtype: int64



### STEP 2 Exploratory Data Analysis
__________________________________________________


##### Distribution of Price based on cab type
1. The Black SUV and Lux Black XL have the maximum cab fares. 
2. Shared and UberPool are the least expensive.
3. The remaining cab types are have neither the highest nor the lowest cab fare.
4. Density of Ubers are more than Lyft.

Conclusions/Assumptions
1. Customers might use these Black SUV and Lux Black XL for long distances. 
2. The remaining cab types are mainly taken to travel within the city. 
3. Uber Pool might have been used for a longer distance than Shared.
4. Ubers are used more than Uber.


```python
sns.stripplot(data=uber_dataset, x='price', y='name')
plt.xlabel('Price')
plt.ylabel('Cab type')
```




    Text(0, 0.5, 'Cab type')




    
![png](output_19_1.png)
    


##### Distribution of price based on weather conditions
1. Price distribution is highest on a clear night than a clear day.
2. Price distribution is highest on cloudy days than rainy days.
3. Price distribution is lower on rainy and foggy days
4. The density is high upto 60 USD after which it is tapered.

Conclusions/Assumptions
1. Customers travel shorter distances on rainy or foggy days
2. Customers take cabs at night to travel long distances. The traffic is higher.
3. Customers travel within the city no matter the weather conditions.
4. Some customers travel long distances during cloudy days/nights or the wait time is higher


```python
#strip plot to find the range of price based on weather conditiom 
sns.stripplot(data=uber_dataset, x='price', y='icon')
plt.xlabel('Price')
plt.ylabel('Weather Conditions')
```




    Text(0, 0.5, 'Weather Conditions')




    
![png](output_21_1.png)
    


##### Price ditribution with distance
The graph is not linear. Ideally, price should increase with distance, but in this case it does not. We assume that due to heavy traffic or bad weather, the wait time is higher and that increases the fares.


```python
x=np.array(uber_dataset['price'])
y=np.array(uber_dataset['distance'])
plt.scatter(x,y,s=1)
plt.xlabel('price')
plt.ylabel('distance')
```




    Text(0, 0.5, 'distance')




    
![png](output_23_1.png)
    


##### Hour vs Price
1. Cabs run at all hours of the day with equal density
2. The highest cab fare was at 1 AM
3. Higher cab fares are noticed between 8PM AND 8AM, with lower density


```python
x=np.array(uber_dataset['hour'])
y=np.array(uber_dataset['price'])
plt.scatter(x,y,s=1)
plt.xlabel('hour')
plt.ylabel('price')
```




    Text(0, 0.5, 'price')




    
![png](output_25_1.png)
    


Handling DateTime Columns


```python
uber_dataset['timestamp'].head()
```




    0    1.544953e+09
    1    1.543284e+09
    2    1.543367e+09
    3    1.543554e+09
    4    1.543463e+09
    Name: timestamp, dtype: float64




```python
from datetime import datetime
timestamp1 = 1544952608
timestamp2 = 1543284024
timestamp3 = 1543818483
timestamp4 = 1543594384
timestamp5 = 1544728504
dt_object1 = datetime.fromtimestamp(timestamp1)
dt_object2 = datetime.fromtimestamp(timestamp2)
dt_object3 = datetime.fromtimestamp(timestamp3)
dt_object4 = datetime.fromtimestamp(timestamp4)
dt_object5 = datetime.fromtimestamp(timestamp5)

print("dt_object =", dt_object1)
print("dt_object =", dt_object2)
print("dt_object =", dt_object3)
print("dt_object =", dt_object4)
print("dt_object =", dt_object5)
```

    dt_object = 2018-12-16 04:30:08
    dt_object = 2018-11-26 21:00:24
    dt_object = 2018-12-03 01:28:03
    dt_object = 2018-11-30 11:13:04
    dt_object = 2018-12-13 14:15:04


It is observed that this data consists of total rides in November and December. There were more rides in December, probably due to the holiday season.


```python
uber_dataset['month'].value_counts().plot(kind='bar', figsize=(10,5), color='blue')
plt.xlabel("Month")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_30_1.png)
    


##### Number of people who take cabs from each location
Equal number of people take cabs from each source and destination


```python
uber_dataset['source'].value_counts().plot(kind='bar', figsize=(10,5), color='green')
plt.xlabel("Pick up point")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_32_1.png)
    



```python
uber_dataset['destination'].value_counts().plot(kind='bar', figsize=(10,5), color='red')
plt.xlabel("Destination")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_33_1.png)
    


##### Uber vs Number of customers
More number of people take Ubers than Lyfts. There is nore trust in Uber, probably because Lyft is relatively newer to the market.


```python
uber_dataset['name'].value_counts().plot(kind='bar', figsize=(10,5), color='orange')
plt.xlabel("Cab type")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_35_1.png)
    


##### Weather conditions vs Customer Count
People take most cabs on cloudy days than rainy days.
1. November and December are cloudy most of the days
2. Customers take cabs mostly on cloudy days than any other day


```python
uber_dataset['icon'].value_counts().plot(kind='bar', figsize=(10,5), color='red')
plt.xlabel("Weather conditions")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_37_1.png)
    


##### Number of customers per weekday
Monday and Tuesday have the highest number of customers


```python
uber_dataset['day-of-week'] = pd.to_datetime(uber_dataset['datetime']).dt.day_name()
```


```python
uber_dataset.groupby('day-of-week')['id'].count().plot(kind='bar')
plt.xlabel('Day of week')
plt.ylabel('Number of customers')
```




    Text(0, 0.5, 'Number of customers')




    
![png](output_40_1.png)
    



```python
## rough work 
uber_dataset.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>month</th>
      <th>datetime</th>
      <th>timezone</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>...</th>
      <th>uvIndexTime</th>
      <th>temperatureMin</th>
      <th>temperatureMinTime</th>
      <th>temperatureMax</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMin</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMax</th>
      <th>apparentTemperatureMaxTime</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>
      <td>1.544953e+09</td>
      <td>9</td>
      <td>16</td>
      <td>12</td>
      <td>2018-12-16 09:30:07</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>1544979600</td>
      <td>39.89</td>
      <td>1545012000</td>
      <td>43.68</td>
      <td>1544968800</td>
      <td>33.73</td>
      <td>1545012000</td>
      <td>38.07</td>
      <td>1544958000</td>
      <td>Sunday</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>
      <td>1.543284e+09</td>
      <td>2</td>
      <td>27</td>
      <td>11</td>
      <td>2018-11-27 02:00:23</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>1543251600</td>
      <td>40.49</td>
      <td>1543233600</td>
      <td>47.30</td>
      <td>1543251600</td>
      <td>36.20</td>
      <td>1543291200</td>
      <td>43.92</td>
      <td>1543251600</td>
      <td>Tuesday</td>
    </tr>
    <tr>
      <th>2</th>
      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>
      <td>1.543367e+09</td>
      <td>1</td>
      <td>28</td>
      <td>11</td>
      <td>2018-11-28 01:00:22</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>1543338000</td>
      <td>35.36</td>
      <td>1543377600</td>
      <td>47.55</td>
      <td>1543320000</td>
      <td>31.04</td>
      <td>1543377600</td>
      <td>44.12</td>
      <td>1543320000</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>3</th>
      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>
      <td>1.543554e+09</td>
      <td>4</td>
      <td>30</td>
      <td>11</td>
      <td>2018-11-30 04:53:02</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>1543507200</td>
      <td>34.67</td>
      <td>1543550400</td>
      <td>45.03</td>
      <td>1543510800</td>
      <td>30.30</td>
      <td>1543550400</td>
      <td>38.53</td>
      <td>1543510800</td>
      <td>Friday</td>
    </tr>
    <tr>
      <th>4</th>
      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>
      <td>1.543463e+09</td>
      <td>3</td>
      <td>29</td>
      <td>11</td>
      <td>2018-11-29 03:49:20</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>1543420800</td>
      <td>33.10</td>
      <td>1543402800</td>
      <td>42.18</td>
      <td>1543420800</td>
      <td>29.11</td>
      <td>1543392000</td>
      <td>35.75</td>
      <td>1543420800</td>
      <td>Thursday</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 58 columns</p>
</div>




```python
from sklearn import preprocessing
```

### STEP 3 Data Preparation
-----------------------------------------------------

### Seperating Data for Normalization and Encoding

All the continuous variables are seperated and normalised using MinMax Scaler and the categroical and object variables are divided into classes.


```python
columnNames = uber_dataset.columns
continuousColumns = []
for i in columnNames:
    if uber_dataset.dtypes[i] == 'float64':
        continuousColumns.append(i)
    else:
        pass
print(continuousColumns)
continuousColumns.remove('timestamp')
print(continuousColumns)
```

    ['timestamp', 'price', 'distance', 'surge_multiplier', 'latitude', 'longitude', 'temperature', 'apparentTemperature', 'precipIntensity', 'precipProbability', 'humidity', 'windSpeed', 'windGust', 'visibility', 'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh', 'apparentTemperatureLow', 'dewPoint', 'pressure', 'cloudCover', 'visibility.1', 'ozone', 'moonPhase', 'precipIntensityMax', 'temperatureMin', 'temperatureMax', 'apparentTemperatureMin', 'apparentTemperatureMax']
    ['price', 'distance', 'surge_multiplier', 'latitude', 'longitude', 'temperature', 'apparentTemperature', 'precipIntensity', 'precipProbability', 'humidity', 'windSpeed', 'windGust', 'visibility', 'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh', 'apparentTemperatureLow', 'dewPoint', 'pressure', 'cloudCover', 'visibility.1', 'ozone', 'moonPhase', 'precipIntensityMax', 'temperatureMin', 'temperatureMax', 'apparentTemperatureMin', 'apparentTemperatureMax']



```python
#Dataset for Normalization
uber_dataset_normal = uber_dataset[continuousColumns]
```


```python
uber_dataset_normal.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>apparentTemperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>...</th>
      <th>pressure</th>
      <th>cloudCover</th>
      <th>visibility.1</th>
      <th>ozone</th>
      <th>moonPhase</th>
      <th>precipIntensityMax</th>
      <th>temperatureMin</th>
      <th>temperatureMax</th>
      <th>apparentTemperatureMin</th>
      <th>apparentTemperatureMax</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.0</td>
      <td>0.44</td>
      <td>1.0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>37.12</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>...</td>
      <td>1021.98</td>
      <td>0.72</td>
      <td>10.000</td>
      <td>303.8</td>
      <td>0.30</td>
      <td>0.1276</td>
      <td>39.89</td>
      <td>43.68</td>
      <td>33.73</td>
      <td>38.07</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11.0</td>
      <td>0.44</td>
      <td>1.0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>37.35</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>...</td>
      <td>1003.97</td>
      <td>1.00</td>
      <td>4.786</td>
      <td>291.1</td>
      <td>0.64</td>
      <td>0.1300</td>
      <td>40.49</td>
      <td>47.30</td>
      <td>36.20</td>
      <td>43.92</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.0</td>
      <td>0.44</td>
      <td>1.0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>32.93</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>...</td>
      <td>992.28</td>
      <td>0.03</td>
      <td>10.000</td>
      <td>315.7</td>
      <td>0.68</td>
      <td>0.1064</td>
      <td>35.36</td>
      <td>47.55</td>
      <td>31.04</td>
      <td>44.12</td>
    </tr>
    <tr>
      <th>3</th>
      <td>26.0</td>
      <td>0.44</td>
      <td>1.0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>29.63</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>...</td>
      <td>1013.73</td>
      <td>0.00</td>
      <td>10.000</td>
      <td>291.1</td>
      <td>0.75</td>
      <td>0.0000</td>
      <td>34.67</td>
      <td>45.03</td>
      <td>30.30</td>
      <td>38.53</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9.0</td>
      <td>0.44</td>
      <td>1.0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>30.88</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>...</td>
      <td>998.36</td>
      <td>0.44</td>
      <td>10.000</td>
      <td>347.7</td>
      <td>0.72</td>
      <td>0.0001</td>
      <td>33.10</td>
      <td>42.18</td>
      <td>29.11</td>
      <td>35.75</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 28 columns</p>
</div>




```python
#Dataset for label encoding
uber_dataset_obj_features = uber_dataset.drop(continuousColumns,axis =1)

```


```python
uber_dataset_obj_features.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>month</th>
      <th>datetime</th>
      <th>timezone</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>...</th>
      <th>windBearing</th>
      <th>uvIndex</th>
      <th>sunriseTime</th>
      <th>sunsetTime</th>
      <th>uvIndexTime</th>
      <th>temperatureMinTime</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMaxTime</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>424553bb-7174-41ea-aeb4-fe06d4f4b9d7</td>
      <td>1.544953e+09</td>
      <td>9</td>
      <td>16</td>
      <td>12</td>
      <td>2018-12-16 09:30:07</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>57</td>
      <td>0</td>
      <td>1544962084</td>
      <td>1544994864</td>
      <td>1544979600</td>
      <td>1545012000</td>
      <td>1544968800</td>
      <td>1545012000</td>
      <td>1544958000</td>
      <td>Sunday</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4bd23055-6827-41c6-b23b-3c491f24e74d</td>
      <td>1.543284e+09</td>
      <td>2</td>
      <td>27</td>
      <td>11</td>
      <td>2018-11-27 02:00:23</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>90</td>
      <td>0</td>
      <td>1543232969</td>
      <td>1543266992</td>
      <td>1543251600</td>
      <td>1543233600</td>
      <td>1543251600</td>
      <td>1543291200</td>
      <td>1543251600</td>
      <td>Tuesday</td>
    </tr>
    <tr>
      <th>2</th>
      <td>981a3613-77af-4620-a42a-0c0866077d1e</td>
      <td>1.543367e+09</td>
      <td>1</td>
      <td>28</td>
      <td>11</td>
      <td>2018-11-28 01:00:22</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>240</td>
      <td>0</td>
      <td>1543319437</td>
      <td>1543353364</td>
      <td>1543338000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>Wednesday</td>
    </tr>
    <tr>
      <th>3</th>
      <td>c2d88af2-d278-4bfd-a8d0-29ca77cc5512</td>
      <td>1.543554e+09</td>
      <td>4</td>
      <td>30</td>
      <td>11</td>
      <td>2018-11-30 04:53:02</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>310</td>
      <td>0</td>
      <td>1543492370</td>
      <td>1543526114</td>
      <td>1543507200</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>Friday</td>
    </tr>
    <tr>
      <th>4</th>
      <td>e0126e1f-8ca9-4f2e-82b3-50505a09db9a</td>
      <td>1.543463e+09</td>
      <td>3</td>
      <td>29</td>
      <td>11</td>
      <td>2018-11-29 03:49:20</td>
      <td>America/New_York</td>
      <td>Haymarket Square</td>
      <td>North Station</td>
      <td>Lyft</td>
      <td>...</td>
      <td>303</td>
      <td>0</td>
      <td>1543405904</td>
      <td>1543439738</td>
      <td>1543420800</td>
      <td>1543402800</td>
      <td>1543420800</td>
      <td>1543392000</td>
      <td>1543420800</td>
      <td>Thursday</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 30 columns</p>
</div>



#### Data Normalization

Due to the large number of features, the algorithms tend to perform better or converge faster when the different features (variables) are on a smaller scale. Therefore the data is normalized before training machine learning models on it.


```python
names = uber_dataset_normal.columns
print(names)
```

    Index(['price', 'distance', 'surge_multiplier', 'latitude', 'longitude',
           'temperature', 'apparentTemperature', 'precipIntensity',
           'precipProbability', 'humidity', 'windSpeed', 'windGust', 'visibility',
           'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh',
           'apparentTemperatureLow', 'dewPoint', 'pressure', 'cloudCover',
           'visibility.1', 'ozone', 'moonPhase', 'precipIntensityMax',
           'temperatureMin', 'temperatureMax', 'apparentTemperatureMin',
           'apparentTemperatureMax'],
          dtype='object')



```python
scaler = preprocessing.MinMaxScaler()

d = scaler.fit_transform(uber_dataset_normal)
scaled_df = pd.DataFrame(d, columns=names)
scaled_df.head() # Normalized Dataset
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>apparentTemperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>...</th>
      <th>pressure</th>
      <th>cloudCover</th>
      <th>visibility.1</th>
      <th>ozone</th>
      <th>moonPhase</th>
      <th>precipIntensityMax</th>
      <th>temperatureMin</th>
      <th>temperatureMax</th>
      <th>apparentTemperatureMin</th>
      <th>apparentTemperatureMax</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.026316</td>
      <td>0.053571</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.611590</td>
      <td>0.554225</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.517241</td>
      <td>...</td>
      <td>0.714075</td>
      <td>0.72</td>
      <td>1.000000</td>
      <td>0.314155</td>
      <td>0.250000</td>
      <td>0.874572</td>
      <td>0.883145</td>
      <td>0.417488</td>
      <td>0.776204</td>
      <td>0.322832</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.089474</td>
      <td>0.053571</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.643957</td>
      <td>0.559326</td>
      <td>0.897719</td>
      <td>1.0</td>
      <td>0.965517</td>
      <td>...</td>
      <td>0.334598</td>
      <td>1.00</td>
      <td>0.438328</td>
      <td>0.198174</td>
      <td>0.654762</td>
      <td>0.891021</td>
      <td>0.904987</td>
      <td>0.566092</td>
      <td>0.863669</td>
      <td>0.529912</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.047368</td>
      <td>0.053571</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.506917</td>
      <td>0.461300</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.637931</td>
      <td>...</td>
      <td>0.088285</td>
      <td>0.03</td>
      <td>1.000000</td>
      <td>0.422831</td>
      <td>0.702381</td>
      <td>0.729267</td>
      <td>0.718238</td>
      <td>0.576355</td>
      <td>0.680949</td>
      <td>0.536991</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.247368</td>
      <td>0.053571</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.403811</td>
      <td>0.388113</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.603448</td>
      <td>...</td>
      <td>0.540244</td>
      <td>0.00</td>
      <td>1.000000</td>
      <td>0.198174</td>
      <td>0.785714</td>
      <td>0.000000</td>
      <td>0.693120</td>
      <td>0.472906</td>
      <td>0.654745</td>
      <td>0.339115</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.068421</td>
      <td>0.053571</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.483686</td>
      <td>0.415835</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.551724</td>
      <td>...</td>
      <td>0.216393</td>
      <td>0.44</td>
      <td>1.000000</td>
      <td>0.715068</td>
      <td>0.750000</td>
      <td>0.000685</td>
      <td>0.635967</td>
      <td>0.355911</td>
      <td>0.612606</td>
      <td>0.240708</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 28 columns</p>
</div>



### Label Encoding


```python
# label_encoder object knows how to understand word labels. 
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder() 
```

Below are the data types of each attribute before Label Encoding. The ML models are mathematical and cannot take string directly. We assign a number to each attribute so that the whole dataset is encoded and the ML model can form a mathematical correlation amongst the attributes.


```python
uber_dataset_obj_features.dtypes
```




    id                              object
    timestamp                      float64
    hour                             int64
    day                              int64
    month                            int64
    datetime                        object
    timezone                        object
    source                          object
    destination                     object
    cab_type                        object
    product_id                      object
    name                            object
    short_summary                   object
    long_summary                    object
    windGustTime                     int64
    temperatureHighTime              int64
    temperatureLowTime               int64
    apparentTemperatureHighTime      int64
    apparentTemperatureLowTime       int64
    icon                            object
    windBearing                      int64
    uvIndex                          int64
    sunriseTime                      int64
    sunsetTime                       int64
    uvIndexTime                      int64
    temperatureMinTime               int64
    temperatureMaxTime               int64
    apparentTemperatureMinTime       int64
    apparentTemperatureMaxTime       int64
    day-of-week                     object
    dtype: object



Encoding all the columns


```python
uber_dataset_obj_features['id']= label_encoder.fit_transform(uber_dataset_obj_features['id']) 
uber_dataset_obj_features['datetime']= label_encoder.fit_transform(uber_dataset_obj_features['datetime']) 
uber_dataset_obj_features['timezone']= label_encoder.fit_transform(uber_dataset_obj_features['timezone'])
uber_dataset_obj_features['destination']= label_encoder.fit_transform(uber_dataset_obj_features['destination']) 
uber_dataset_obj_features['product_id']= label_encoder.fit_transform(uber_dataset_obj_features['product_id'])
uber_dataset_obj_features['short_summary']= label_encoder.fit_transform(uber_dataset_obj_features['short_summary'])
uber_dataset_obj_features['long_summary']= label_encoder.fit_transform(uber_dataset_obj_features['long_summary'])
uber_dataset_obj_features['day-of-week']= label_encoder.fit_transform(uber_dataset_obj_features['day-of-week'])
```


```python
uber_dataset_obj_features['cab_type'].unique()
```




    array(['Lyft', 'Uber'], dtype=object)




```python
uber_dataset_obj_features['cab_type']= label_encoder.fit_transform(uber_dataset_obj_features['cab_type'])

print("Class mapping of cab_type: ")
for i, item in enumerate(label_encoder.classes_):
    print(item, "-->", i)
```

    Class mapping of cab_type: 
    Lyft --> 0
    Uber --> 1



```python
uber_dataset_obj_features['name']= label_encoder.fit_transform(uber_dataset_obj_features['name'])

print("Class mapping of Name: ")
for i, item in enumerate(label_encoder.classes_):
    print(item, "-->", i)
```

    Class mapping of Name: 
    Black --> 0
    Black SUV --> 1
    Lux --> 2
    Lux Black --> 3
    Lux Black XL --> 4
    Lyft --> 5
    Lyft XL --> 6
    Shared --> 7
    Taxi --> 8
    UberPool --> 9
    UberX --> 10
    UberXL --> 11
    WAV --> 12



```python
uber_dataset_obj_features['source']= label_encoder.fit_transform(uber_dataset_obj_features['source'])

print("Class mapping of Source: ")
for i, item in enumerate(label_encoder.classes_):
    print(item, "-->", i)
```

    Class mapping of Source: 
    Back Bay --> 0
    Beacon Hill --> 1
    Boston University --> 2
    Fenway --> 3
    Financial District --> 4
    Haymarket Square --> 5
    North End --> 6
    North Station --> 7
    Northeastern University --> 8
    South Station --> 9
    Theatre District --> 10
    West End --> 11



```python
uber_dataset_obj_features['icon']= label_encoder.fit_transform(uber_dataset_obj_features['icon'])

print("Class mapping of Icon: ")
for i, item in enumerate(label_encoder.classes_):
    print(item, "-->", i)
```

    Class mapping of Icon: 
     clear-day  --> 0
     clear-night  --> 1
     cloudy  --> 2
     fog  --> 3
     partly-cloudy-day  --> 4
     partly-cloudy-night  --> 5
     rain  --> 6


All the encoded columns have changed to int datatype. All those columns are encoded and can be fed into ML models


```python
# checking the data type after label encoding
uber_dataset_obj_features.dtypes
```




    id                               int64
    timestamp                      float64
    hour                             int64
    day                              int64
    month                            int64
    datetime                         int64
    timezone                         int64
    source                           int64
    destination                      int64
    cab_type                         int64
    product_id                       int64
    name                             int64
    short_summary                    int64
    long_summary                     int64
    windGustTime                     int64
    temperatureHighTime              int64
    temperatureLowTime               int64
    apparentTemperatureHighTime      int64
    apparentTemperatureLowTime       int64
    icon                             int64
    windBearing                      int64
    uvIndex                          int64
    sunriseTime                      int64
    sunsetTime                       int64
    uvIndexTime                      int64
    temperatureMinTime               int64
    temperatureMaxTime               int64
    apparentTemperatureMinTime       int64
    apparentTemperatureMaxTime       int64
    day-of-week                      int64
    dtype: object




```python
uber_dataset_obj_features.head()  #Encoded Dataset 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>month</th>
      <th>datetime</th>
      <th>timezone</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>...</th>
      <th>windBearing</th>
      <th>uvIndex</th>
      <th>sunriseTime</th>
      <th>sunsetTime</th>
      <th>uvIndexTime</th>
      <th>temperatureMinTime</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMaxTime</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>179271</td>
      <td>1.544953e+09</td>
      <td>9</td>
      <td>16</td>
      <td>12</td>
      <td>25351</td>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>...</td>
      <td>57</td>
      <td>0</td>
      <td>1544962084</td>
      <td>1544994864</td>
      <td>1544979600</td>
      <td>1545012000</td>
      <td>1544968800</td>
      <td>1545012000</td>
      <td>1544958000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>205021</td>
      <td>1.543284e+09</td>
      <td>2</td>
      <td>27</td>
      <td>11</td>
      <td>961</td>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>...</td>
      <td>90</td>
      <td>0</td>
      <td>1543232969</td>
      <td>1543266992</td>
      <td>1543251600</td>
      <td>1543233600</td>
      <td>1543251600</td>
      <td>1543291200</td>
      <td>1543251600</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>411506</td>
      <td>1.543367e+09</td>
      <td>1</td>
      <td>28</td>
      <td>11</td>
      <td>2534</td>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>...</td>
      <td>240</td>
      <td>0</td>
      <td>1543319437</td>
      <td>1543353364</td>
      <td>1543338000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>527263</td>
      <td>1.543554e+09</td>
      <td>4</td>
      <td>30</td>
      <td>11</td>
      <td>6988</td>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>...</td>
      <td>310</td>
      <td>0</td>
      <td>1543492370</td>
      <td>1543526114</td>
      <td>1543507200</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>606526</td>
      <td>1.543463e+09</td>
      <td>3</td>
      <td>29</td>
      <td>11</td>
      <td>4400</td>
      <td>0</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>...</td>
      <td>303</td>
      <td>0</td>
      <td>1543405904</td>
      <td>1543439738</td>
      <td>1543420800</td>
      <td>1543402800</td>
      <td>1543420800</td>
      <td>1543392000</td>
      <td>1543420800</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 30 columns</p>
</div>



#### Combining the Encoded and the Normalized parts of the dataset together


```python
uber_dataset = uber_dataset_normal.join(uber_dataset_obj_features)
```


```python
uber_dataset['price']
```




    0          5.0
    1         11.0
    2          7.0
    3         26.0
    4          9.0
              ... 
    693066    13.0
    693067     9.5
    693068     NaN
    693069    27.0
    693070    10.0
    Name: price, Length: 693071, dtype: float64




```python
uber_dataset.columns
```




    Index(['price', 'distance', 'surge_multiplier', 'latitude', 'longitude',
           'temperature', 'apparentTemperature', 'precipIntensity',
           'precipProbability', 'humidity', 'windSpeed', 'windGust', 'visibility',
           'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh',
           'apparentTemperatureLow', 'dewPoint', 'pressure', 'cloudCover',
           'visibility.1', 'ozone', 'moonPhase', 'precipIntensityMax',
           'temperatureMin', 'temperatureMax', 'apparentTemperatureMin',
           'apparentTemperatureMax', 'id', 'timestamp', 'hour', 'day', 'month',
           'datetime', 'timezone', 'source', 'destination', 'cab_type',
           'product_id', 'name', 'short_summary', 'long_summary', 'windGustTime',
           'temperatureHighTime', 'temperatureLowTime',
           'apparentTemperatureHighTime', 'apparentTemperatureLowTime', 'icon',
           'windBearing', 'uvIndex', 'sunriseTime', 'sunsetTime', 'uvIndexTime',
           'temperatureMinTime', 'temperatureMaxTime',
           'apparentTemperatureMinTime', 'apparentTemperatureMaxTime',
           'day-of-week'],
          dtype='object')



### Binning

We are classifying the values of the surge multiplier into 5 buckets numbered from 0 to 4.

Surge multiplier is the value determined by the wait time of the cab - wether in traffic or before pick up - multiplied to the actual price of the cab fare. This is an indirect representation of the wait time.


```python
uber_dataset['surge_multiplier'].unique()
```




    array([1.  , 1.25, 2.5 , 2.  , 1.75, 1.5 , 3.  ])




```python
surge_multiplier_mapping = {1.: 0, 1.25: 1, 1.5: 2, 1.75: 3, 2.:4, 2.5:5, 3.:6}
uber_dataset['surge_multiplier'] = uber_dataset['surge_multiplier'].map(surge_multiplier_mapping)
```

In the next 3 cells we find NAN values in the dataset and replace them with the median value as standard practice Data Science practice.


```python
uber_dataset.isnull().sum()
```




    price                          55095
    distance                           0
    surge_multiplier                   0
    latitude                           0
    longitude                          0
    temperature                        0
    apparentTemperature                0
    precipIntensity                    0
    precipProbability                  0
    humidity                           0
    windSpeed                          0
    windGust                           0
    visibility                         0
    temperatureHigh                    0
    temperatureLow                     0
    apparentTemperatureHigh            0
    apparentTemperatureLow             0
    dewPoint                           0
    pressure                           0
    cloudCover                         0
    visibility.1                       0
    ozone                              0
    moonPhase                          0
    precipIntensityMax                 0
    temperatureMin                     0
    temperatureMax                     0
    apparentTemperatureMin             0
    apparentTemperatureMax             0
    id                                 0
    timestamp                          0
    hour                               0
    day                                0
    month                              0
    datetime                           0
    timezone                           0
    source                             0
    destination                        0
    cab_type                           0
    product_id                         0
    name                               0
    short_summary                      0
    long_summary                       0
    windGustTime                       0
    temperatureHighTime                0
    temperatureLowTime                 0
    apparentTemperatureHighTime        0
    apparentTemperatureLowTime         0
    icon                               0
    windBearing                        0
    uvIndex                            0
    sunriseTime                        0
    sunsetTime                         0
    uvIndexTime                        0
    temperatureMinTime                 0
    temperatureMaxTime                 0
    apparentTemperatureMinTime         0
    apparentTemperatureMaxTime         0
    day-of-week                        0
    dtype: int64




```python
uber_dataset['price'].median()
```




    13.5




```python
uber_dataset["price"].fillna(13.5, inplace = True) 
```


```python
uber_dataset.isnull().sum()
```




    price                          0
    distance                       0
    surge_multiplier               0
    latitude                       0
    longitude                      0
    temperature                    0
    apparentTemperature            0
    precipIntensity                0
    precipProbability              0
    humidity                       0
    windSpeed                      0
    windGust                       0
    visibility                     0
    temperatureHigh                0
    temperatureLow                 0
    apparentTemperatureHigh        0
    apparentTemperatureLow         0
    dewPoint                       0
    pressure                       0
    cloudCover                     0
    visibility.1                   0
    ozone                          0
    moonPhase                      0
    precipIntensityMax             0
    temperatureMin                 0
    temperatureMax                 0
    apparentTemperatureMin         0
    apparentTemperatureMax         0
    id                             0
    timestamp                      0
    hour                           0
    day                            0
    month                          0
    datetime                       0
    timezone                       0
    source                         0
    destination                    0
    cab_type                       0
    product_id                     0
    name                           0
    short_summary                  0
    long_summary                   0
    windGustTime                   0
    temperatureHighTime            0
    temperatureLowTime             0
    apparentTemperatureHighTime    0
    apparentTemperatureLowTime     0
    icon                           0
    windBearing                    0
    uvIndex                        0
    sunriseTime                    0
    sunsetTime                     0
    uvIndexTime                    0
    temperatureMinTime             0
    temperatureMaxTime             0
    apparentTemperatureMinTime     0
    apparentTemperatureMaxTime     0
    day-of-week                    0
    dtype: int64




```python
uber_dataset['price'].dtype
```




    dtype('float64')




```python
uber_dataset['price'] = uber_dataset['price'].astype(int)
```


```python
uber_dataset['price'].head()
```




    0     5
    1    11
    2     7
    3    26
    4     9
    Name: price, dtype: int64




```python
# price = pd.DataFrame(price)
```

### STEP 4 RFE (Recursive Feature Elimination)

Our goal here is to find a correlation amongst the attributes of the dataset. Since we want to predict the price of the cab fare, we need to find a correlation between the price and the remaining attributes like weather conditions, time of day etc.

So we assign price to the y which is the dependent variable, and the influencing attributes to X. 

Mathematically, an algebriac equation can be defined between X and y, based on the reccurring patterns of the encoded values.


```python
X = uber_dataset.drop('price', axis = 1)
y = uber_dataset['price']
```


```python
X.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>apparentTemperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>windSpeed</th>
      <th>...</th>
      <th>windBearing</th>
      <th>uvIndex</th>
      <th>sunriseTime</th>
      <th>sunsetTime</th>
      <th>uvIndexTime</th>
      <th>temperatureMinTime</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMaxTime</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>37.12</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>8.66</td>
      <td>...</td>
      <td>57</td>
      <td>0</td>
      <td>1544962084</td>
      <td>1544994864</td>
      <td>1544979600</td>
      <td>1545012000</td>
      <td>1544968800</td>
      <td>1545012000</td>
      <td>1544958000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>37.35</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>11.98</td>
      <td>...</td>
      <td>90</td>
      <td>0</td>
      <td>1543232969</td>
      <td>1543266992</td>
      <td>1543251600</td>
      <td>1543233600</td>
      <td>1543251600</td>
      <td>1543291200</td>
      <td>1543251600</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>32.93</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>7.33</td>
      <td>...</td>
      <td>240</td>
      <td>0</td>
      <td>1543319437</td>
      <td>1543353364</td>
      <td>1543338000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>29.63</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>5.28</td>
      <td>...</td>
      <td>310</td>
      <td>0</td>
      <td>1543492370</td>
      <td>1543526114</td>
      <td>1543507200</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>30.88</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>9.14</td>
      <td>...</td>
      <td>303</td>
      <td>0</td>
      <td>1543405904</td>
      <td>1543439738</td>
      <td>1543420800</td>
      <td>1543402800</td>
      <td>1543420800</td>
      <td>1543392000</td>
      <td>1543420800</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 57 columns</p>
</div>




```python
y.head()
```




    0     5
    1    11
    2     7
    3    26
    4     9
    Name: price, dtype: int64




```python
X.shape
```




    (693071, 57)




```python
y.shape
```




    (693071,)



The below graph shows the frequency of the price in November and December. 13 USD is the most frequent cab fare and 97 USD is the least frequent cab fare.


```python
y.value_counts().plot(kind='bar',figsize=(30,8),color='red')
plt.xlabel("Price")
plt.ylabel("Total count")
```




    Text(0, 0.5, 'Total count')




    
![png](output_92_1.png)
    


#### Splitting the data set into training set and test set for training and validation of the ML Models


```python
breakpoint
```




    <function breakpoint>




```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
```


```python
X_train.shape
```




    (554456, 57)




```python
X_test.shape
```




    (138615, 57)




```python
y_train.shape
```




    (554456,)




```python
y_test.shape
```




    (138615,)



##### Understanding the influence of each feature on price

Since this is a regression model, to determine the importance of each feature, the model assigns a coefficient as the score. Below is the assigned score for each feature:


```python
#Creating model
reg = LinearRegression()
#Fitting training data
reg = reg.fit(X_train, y_train)
```


```python
importance = reg.coef_
for i,v in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()
```

    Feature: 0, Score: 2.55669
    Feature: 1, Score: 5.13461
    Feature: 2, Score: 1.00017
    Feature: 3, Score: 1.15865
    Feature: 4, Score: -0.01015
    Feature: 5, Score: 0.00302
    Feature: 6, Score: -0.44246
    Feature: 7, Score: -0.01040
    Feature: 8, Score: -0.15044
    Feature: 9, Score: -0.00884
    Feature: 10, Score: 0.00533
    Feature: 11, Score: -0.00155
    Feature: 12, Score: -0.17282
    Feature: 13, Score: -0.03111
    Feature: 14, Score: 0.09782
    Feature: 15, Score: 0.02252
    Feature: 16, Score: 0.00467
    Feature: 17, Score: -0.00036
    Feature: 18, Score: 0.02636
    Feature: 19, Score: -0.00155
    Feature: 20, Score: -0.00131
    Feature: 21, Score: 0.02643
    Feature: 22, Score: -0.44830
    Feature: 23, Score: -0.02517
    Feature: 24, Score: 0.18028
    Feature: 25, Score: 0.01865
    Feature: 26, Score: -0.09875
    Feature: 27, Score: 0.00000
    Feature: 28, Score: -0.00001
    Feature: 29, Score: 0.01936
    Feature: 30, Score: 0.45864
    Feature: 31, Score: 13.84267
    Feature: 32, Score: 0.00001
    Feature: 33, Score: 0.00000
    Feature: 34, Score: 0.03336
    Feature: 35, Score: 0.00898
    Feature: 36, Score: 7.57630
    Feature: 37, Score: 0.68070
    Feature: 38, Score: -1.58560
    Feature: 39, Score: -0.00593
    Feature: 40, Score: -0.01370
    Feature: 41, Score: 0.00000
    Feature: 42, Score: 0.00000
    Feature: 43, Score: 0.00000
    Feature: 44, Score: -0.00000
    Feature: 45, Score: -0.00000
    Feature: 46, Score: 0.00632
    Feature: 47, Score: 0.00026
    Feature: 48, Score: 0.00132
    Feature: 49, Score: -0.00227
    Feature: 50, Score: 0.00228
    Feature: 51, Score: -0.00001
    Feature: 52, Score: -0.00000
    Feature: 53, Score: 0.00000
    Feature: 54, Score: 0.00000
    Feature: 55, Score: 0.00000
    Feature: 56, Score: 0.00481



    
![png](output_103_1.png)
    


The important features have the maximum score as listed below:

	Feature: 3, Score: 0.45864
	Feature: 4, Score: 13.84267
	Feature: 7, Score: 0.03336
	Feature: 9, Score: 7.57630
	Feature: 10, Score: 0.68070
	Feature: 12, Score: 2.55669
	Feature: 13, Score: 5.13461
	Feature: 14, Score: 1.00017
	Feature: 15, Score: 1.15865
	Feature: 35, Score: 0.00632
	Feature: 50, Score: 0.18028
	Feature: 52, Score: 0.01865
	Feature: 56, Score: 0.00481


```python
print(X.columns[3])
print(X.columns[4])
print(X.columns[7])
print(X.columns[9])
print(X.columns[10])
print(X.columns[12])
print(X.columns[13])
print(X.columns[14])
print(X.columns[15])
print(X.columns[35])
print(X.columns[50])
print(X.columns[56])
```

    longitude
    temperature
    precipProbability
    windSpeed
    windGust
    temperatureHigh
    temperatureLow
    apparentTemperatureHigh
    apparentTemperatureLow
    destination
    sunsetTime
    day-of-week


#### Testing the training accuracy with all 56 features


```python
rfe = RFE(reg,n_features_to_select=56, verbose=1)
rfe = rfe.fit(X, y)
```

    Fitting estimator with 57 features.



```python
rfe.support_
```




    array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True, False,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True])




```python
XX = X[X.columns[rfe.support_]]
```


```python
XX.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>apparentTemperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>windSpeed</th>
      <th>...</th>
      <th>windBearing</th>
      <th>uvIndex</th>
      <th>sunriseTime</th>
      <th>sunsetTime</th>
      <th>uvIndexTime</th>
      <th>temperatureMinTime</th>
      <th>temperatureMaxTime</th>
      <th>apparentTemperatureMinTime</th>
      <th>apparentTemperatureMaxTime</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>37.12</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>8.66</td>
      <td>...</td>
      <td>57</td>
      <td>0</td>
      <td>1544962084</td>
      <td>1544994864</td>
      <td>1544979600</td>
      <td>1545012000</td>
      <td>1544968800</td>
      <td>1545012000</td>
      <td>1544958000</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>37.35</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>11.98</td>
      <td>...</td>
      <td>90</td>
      <td>0</td>
      <td>1543232969</td>
      <td>1543266992</td>
      <td>1543251600</td>
      <td>1543233600</td>
      <td>1543251600</td>
      <td>1543291200</td>
      <td>1543251600</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>32.93</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>7.33</td>
      <td>...</td>
      <td>240</td>
      <td>0</td>
      <td>1543319437</td>
      <td>1543353364</td>
      <td>1543338000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>1543377600</td>
      <td>1543320000</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>29.63</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>5.28</td>
      <td>...</td>
      <td>310</td>
      <td>0</td>
      <td>1543492370</td>
      <td>1543526114</td>
      <td>1543507200</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>1543550400</td>
      <td>1543510800</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>30.88</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>9.14</td>
      <td>...</td>
      <td>303</td>
      <td>0</td>
      <td>1543405904</td>
      <td>1543439738</td>
      <td>1543420800</td>
      <td>1543402800</td>
      <td>1543420800</td>
      <td>1543392000</td>
      <td>1543420800</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 56 columns</p>
</div>




```python
X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.3, random_state = 10)
```


```python
X_train.shape
```




    (485149, 56)




```python
#Creating model
reg1 = LinearRegression()
#Fitting training data
reg1 = reg1.fit(X_train, y_train)
```


```python
reg1.score(X_train, y_train)
```




    0.5264086776246024



#### Testing the training accuracy with 40 features


```python
rfe = RFE(reg,n_features_to_select= 40,step=1)
rfe = rfe.fit(X, y)
```


```python
rfe.support_
```




    array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
           False, False,  True,  True,  True, False, False,  True,  True,
            True,  True,  True,  True,  True, False, False, False, False,
           False,  True, False,  True, False, False, False, False, False,
           False, False,  True])




```python
XX = X[X.columns[rfe.support_]]
```


```python
#Final Dateset after preprocessing 
XX.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>apparentTemperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>windSpeed</th>
      <th>...</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>short_summary</th>
      <th>long_summary</th>
      <th>icon</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>37.12</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>8.66</td>
      <td>...</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>4</td>
      <td>9</td>
      <td>5</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>37.35</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>11.98</td>
      <td>...</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>8</td>
      <td>10</td>
      <td>6</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>32.93</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>7.33</td>
      <td>...</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>29.63</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>5.28</td>
      <td>...</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>6</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>30.88</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>9.14</td>
      <td>...</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>6</td>
      <td>4</td>
      <td>5</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 40 columns</p>
</div>




```python
#Splitting the final data set into test and train
X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.3, random_state = 10)
```


```python
X_train.shape
```




    (485149, 40)




```python
#Creating model
reg1 = LinearRegression()
#Fitting training data
reg1 = reg1.fit(X_train, y_train)
```


```python
reg1.score(X_train, y_train)
```




    0.5264022449568696



#### Testing the training accuracy with 15 features


```python
rfe = RFE(reg,n_features_to_select=15, verbose=1)
rfe = rfe.fit(X, y)
```

    Fitting estimator with 57 features.
    Fitting estimator with 56 features.
    Fitting estimator with 55 features.
    Fitting estimator with 54 features.
    Fitting estimator with 53 features.
    Fitting estimator with 52 features.
    Fitting estimator with 51 features.
    Fitting estimator with 50 features.
    Fitting estimator with 49 features.
    Fitting estimator with 48 features.
    Fitting estimator with 47 features.
    Fitting estimator with 46 features.
    Fitting estimator with 45 features.
    Fitting estimator with 44 features.
    Fitting estimator with 43 features.
    Fitting estimator with 42 features.
    Fitting estimator with 41 features.
    Fitting estimator with 40 features.
    Fitting estimator with 39 features.
    Fitting estimator with 38 features.
    Fitting estimator with 37 features.
    Fitting estimator with 36 features.
    Fitting estimator with 35 features.
    Fitting estimator with 34 features.
    Fitting estimator with 33 features.
    Fitting estimator with 32 features.
    Fitting estimator with 31 features.
    Fitting estimator with 30 features.
    Fitting estimator with 29 features.
    Fitting estimator with 28 features.
    Fitting estimator with 27 features.
    Fitting estimator with 26 features.
    Fitting estimator with 25 features.
    Fitting estimator with 24 features.
    Fitting estimator with 23 features.
    Fitting estimator with 22 features.
    Fitting estimator with 21 features.
    Fitting estimator with 20 features.
    Fitting estimator with 19 features.
    Fitting estimator with 18 features.
    Fitting estimator with 17 features.
    Fitting estimator with 16 features.



```python
XX = X[X.columns[rfe.support_]]
```


```python
XX.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>precipIntensity</th>
      <th>humidity</th>
      <th>temperatureHigh</th>
      <th>apparentTemperatureHigh</th>
      <th>dewPoint</th>
      <th>temperatureMax</th>
      <th>apparentTemperatureMax</th>
      <th>source</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>0.0000</td>
      <td>0.68</td>
      <td>43.68</td>
      <td>37.95</td>
      <td>32.70</td>
      <td>43.68</td>
      <td>38.07</td>
      <td>5</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>0.1299</td>
      <td>0.94</td>
      <td>47.30</td>
      <td>43.92</td>
      <td>41.83</td>
      <td>47.30</td>
      <td>43.92</td>
      <td>5</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>0.0000</td>
      <td>0.75</td>
      <td>47.55</td>
      <td>44.12</td>
      <td>31.10</td>
      <td>47.55</td>
      <td>44.12</td>
      <td>5</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>0.0000</td>
      <td>0.73</td>
      <td>45.03</td>
      <td>38.53</td>
      <td>26.64</td>
      <td>45.03</td>
      <td>38.53</td>
      <td>5</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>0.0000</td>
      <td>0.70</td>
      <td>42.18</td>
      <td>35.75</td>
      <td>28.61</td>
      <td>42.18</td>
      <td>35.75</td>
      <td>5</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>




```python
X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.3, random_state = 10,)
```


```python
X_train.shape
```




    (485149, 15)




```python
#Creating model
reg1 = LinearRegression()
#Fitting training data
reg1 = reg1.fit(X_train, y_train)
```


```python
reg1.score(X_train, y_train)
```




    0.526375236736347



#### Testing the training accuracy with 25 features


```python
rfe = RFE(reg, n_features_to_select=25, verbose=1)
rfe = rfe.fit(X, y)
```

    Fitting estimator with 57 features.
    Fitting estimator with 56 features.
    Fitting estimator with 55 features.
    Fitting estimator with 54 features.
    Fitting estimator with 53 features.
    Fitting estimator with 52 features.
    Fitting estimator with 51 features.
    Fitting estimator with 50 features.
    Fitting estimator with 49 features.
    Fitting estimator with 48 features.
    Fitting estimator with 47 features.
    Fitting estimator with 46 features.
    Fitting estimator with 45 features.
    Fitting estimator with 44 features.
    Fitting estimator with 43 features.
    Fitting estimator with 42 features.
    Fitting estimator with 41 features.
    Fitting estimator with 40 features.
    Fitting estimator with 39 features.
    Fitting estimator with 38 features.
    Fitting estimator with 37 features.
    Fitting estimator with 36 features.
    Fitting estimator with 35 features.
    Fitting estimator with 34 features.
    Fitting estimator with 33 features.
    Fitting estimator with 32 features.
    Fitting estimator with 31 features.
    Fitting estimator with 30 features.
    Fitting estimator with 29 features.
    Fitting estimator with 28 features.
    Fitting estimator with 27 features.
    Fitting estimator with 26 features.



```python
XX = X[X.columns[rfe.support_]]
```


```python
XX.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>temperatureHigh</th>
      <th>temperatureLow</th>
      <th>...</th>
      <th>temperatureMax</th>
      <th>apparentTemperatureMax</th>
      <th>month</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>43.68</td>
      <td>34.19</td>
      <td>...</td>
      <td>43.68</td>
      <td>38.07</td>
      <td>12</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>47.30</td>
      <td>42.10</td>
      <td>...</td>
      <td>47.30</td>
      <td>43.92</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>47.55</td>
      <td>33.10</td>
      <td>...</td>
      <td>47.55</td>
      <td>44.12</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>45.03</td>
      <td>28.90</td>
      <td>...</td>
      <td>45.03</td>
      <td>38.53</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>42.18</td>
      <td>36.71</td>
      <td>...</td>
      <td>42.18</td>
      <td>35.75</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 25 columns</p>
</div>




```python
X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.3, random_state = 20,)
```


```python
X_train.shape
```




    (485149, 25)




```python
#Creating model
reg1 = LinearRegression()
#Fitting training data
reg1 = reg1.fit(X_train, y_train)
#Y prediction
Y_pred = reg1.predict(X_test)
```


```python
reg1.score(X_train, y_train)
```




    0.5262112568701124



#### Columns After RFE


```python
XX.columns
```




    Index(['distance', 'surge_multiplier', 'latitude', 'longitude', 'temperature',
           'precipIntensity', 'precipProbability', 'humidity', 'temperatureHigh',
           'temperatureLow', 'apparentTemperatureHigh', 'dewPoint', 'cloudCover',
           'moonPhase', 'precipIntensityMax', 'temperatureMax',
           'apparentTemperatureMax', 'month', 'source', 'destination', 'cab_type',
           'product_id', 'name', 'uvIndex', 'day-of-week'],
          dtype='object')




```python
XX.shape
```




    (693071, 25)




```python
XX.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>humidity</th>
      <th>temperatureHigh</th>
      <th>temperatureLow</th>
      <th>...</th>
      <th>temperatureMax</th>
      <th>apparentTemperatureMax</th>
      <th>month</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.68</td>
      <td>43.68</td>
      <td>34.19</td>
      <td>...</td>
      <td>43.68</td>
      <td>38.07</td>
      <td>12</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>0.94</td>
      <td>47.30</td>
      <td>42.10</td>
      <td>...</td>
      <td>47.30</td>
      <td>43.92</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>47.55</td>
      <td>33.10</td>
      <td>...</td>
      <td>47.55</td>
      <td>44.12</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.73</td>
      <td>45.03</td>
      <td>28.90</td>
      <td>...</td>
      <td>45.03</td>
      <td>38.53</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>0.70</td>
      <td>42.18</td>
      <td>36.71</td>
      <td>...</td>
      <td>42.18</td>
      <td>35.75</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 25 columns</p>
</div>




```python

```

### STEP 6 Feature Selection


```python
# features_drop = ['precipIntensity', 'precipProbability', 'humidity', 'temperatureHigh',
#        'temperatureLow', 'apparentTemperatureHigh', 'dewPoint','cloudCover',
#        'uvIndex', 'moonPhase', 'precipIntensityMax', 'temperatureMax',
#        'apparentTemperatureMax']
# new_uber = XX.drop(features_drop, axis=1)
```


```python
features_drop = [ 'humidity', 'temperatureHigh', 'apparentTemperatureHigh', 'dewPoint', 'temperatureMax',
       'apparentTemperatureMax']
new_uber = XX.drop(features_drop, axis=1)
```


```python
new_uber.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>temperatureLow</th>
      <th>cloudCover</th>
      <th>moonPhase</th>
      <th>precipIntensityMax</th>
      <th>month</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>34.19</td>
      <td>0.72</td>
      <td>0.30</td>
      <td>0.1276</td>
      <td>12</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>42.10</td>
      <td>1.00</td>
      <td>0.64</td>
      <td>0.1300</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>33.10</td>
      <td>0.03</td>
      <td>0.68</td>
      <td>0.1064</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>28.90</td>
      <td>0.00</td>
      <td>0.75</td>
      <td>0.0000</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>36.71</td>
      <td>0.44</td>
      <td>0.72</td>
      <td>0.0001</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>



#### Final Dataset


```python
new_uber.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>temperatureLow</th>
      <th>cloudCover</th>
      <th>moonPhase</th>
      <th>precipIntensityMax</th>
      <th>month</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>34.19</td>
      <td>0.72</td>
      <td>0.30</td>
      <td>0.1276</td>
      <td>12</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>42.10</td>
      <td>1.00</td>
      <td>0.64</td>
      <td>0.1300</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>33.10</td>
      <td>0.03</td>
      <td>0.68</td>
      <td>0.1064</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>28.90</td>
      <td>0.00</td>
      <td>0.75</td>
      <td>0.0000</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>36.71</td>
      <td>0.44</td>
      <td>0.72</td>
      <td>0.0001</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
new_uber.fillna(0, inplace=True)
```


```python
y.head()
```




    0     5
    1    11
    2     7
    3    26
    4     9
    Name: price, dtype: int64



## STEP 7 Modelling and Testing
-----------------------------------------------------


```python
new_uber.shape
```




    (693071, 19)




```python
y.shape
```




    (693071,)




```python
# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
xx_train, xx_test, yy_train, yy_test = train_test_split(new_uber, y, test_size = 0.2, random_state = 42)
```


```python
xx_train.shape
```




    (554456, 19)




```python
xx_test.shape
```




    (138615, 19)




```python
yy_train.shape
```




    (554456,)




```python
yy_test.shape
```




    (138615,)




```python
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor 
```


```python
new_uber.fillna(X_train.mean(), inplace=True)
```

### Linear regression


```python
from scipy.stats import loguniform
from pandas import read_csv
from sklearn.linear_model import Ridge
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import RandomizedSearchCV
```


```python
linear = LinearRegression(fit_intercept=True, normalize=False,copy_X=True, n_jobs=None)
```

1. Fitting


```python
linear.fit(xx_train, yy_train)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.
      warnings.warn(





<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression(normalize=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" checked><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression(normalize=False)</pre></div></div></div></div></div>



2. Scoring


```python
print('linear_score : ',linear.score(xx_test, yy_test))
linear_score=linear.score(xx_test, yy_test)
```

    linear_score :  0.5249820205831788


#### Testing Linear Regression


```python
prediction = linear.predict(xx_test)
prediction=  prediction.astype(int)
```

1. Actual vs Prediction


```python
plt.scatter(yy_test,prediction)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')
```




    Text(0, 0.5, 'Predicted Y')




    
![png](output_173_1.png)
    


2. Performance metric of the Linear Regression Model- 


```python
from sklearn import metrics
print('MAE :'," ", metrics.mean_absolute_error(yy_test,prediction))
print('MSE :'," ", metrics.mean_squared_error(yy_test,prediction))
print('RMAE :'," ", np.sqrt(metrics.mean_squared_error(yy_test,prediction)))
```

    MAE :   4.786487753850594
    MSE :   38.45307506402626
    RMAE :   6.201054350997599



```python
linear_score=linear.score(xx_test, yy_test)
linear_MAE=metrics.mean_absolute_error(yy_test,prediction)
linear_MSE= metrics.mean_squared_error(yy_test,prediction)
linear_RMAE=np.sqrt(metrics.mean_squared_error(yy_test,prediction))
```


```python
sns.distplot(yy_test - prediction,bins=50)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
      warnings.warn(msg, FutureWarning)





    <AxesSubplot:xlabel='price', ylabel='Density'>




    
![png](output_177_2.png)
    


3. K Cross-validation


```python
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
cross_val_score(LinearRegression(),xx_test,yy_test,cv=cv)
```




    array([0.52811673, 0.52813062, 0.52446251, 0.52223455, 0.52055002])



Conclusion -

Linear regression is not a good model for this data since this data is not linear. This data is too complex for a linear model.

The remaining ML Models are more robust for this data. These models learns the variety of data better.

### Decision Tree

1. Hyper performance tuning


```python
decision = DecisionTreeRegressor(random_state = 0, max_depth=12)
```

2. Fitting


```python
decision.fit(xx_train , yy_train)
```




<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=12, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" checked><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=12, random_state=0)</pre></div></div></div></div></div>



3. Scoring


```python
print('Decision_tree_score :',decision.score(xx_test, yy_test))
```

    Decision_tree_score : 0.96525270449834


#### Testing decision tree

1. Actual vs Predicted values


```python
prediction=decision.predict(xx_test)
```


```python
plt.scatter(yy_test,prediction)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')
```




    Text(0, 0.5, 'Predicted Y')




    
![png](output_192_1.png)
    


2. Performance metric of the Decision tree model


```python
from sklearn import metrics
print('MAE :'," ", metrics.mean_absolute_error(yy_test,prediction))
print('MSE :'," ", metrics.mean_squared_error(yy_test,prediction))
print('RMAE :'," ", np.sqrt(metrics.mean_squared_error(yy_test,prediction)))
```

    MAE :   1.0172356524430664
    MSE :   2.7878175031743164
    RMAE :   1.6696758676983734



```python
decision_score=decision.score(xx_test, yy_test)
decision_MAE=metrics.mean_absolute_error(yy_test,prediction)
decision_MSE= metrics.mean_squared_error(yy_test,prediction)
decision_RMAE=np.sqrt(metrics.mean_squared_error(yy_test,prediction))
```


```python
sns.distplot(yy_test - prediction,bins=50)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
      warnings.warn(msg, FutureWarning)





    <AxesSubplot:xlabel='price', ylabel='Density'>




    
![png](output_196_2.png)
    


3. K Cross-validation for decision tree


```python
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
cross_val_score(DecisionTreeRegressor(),xx_test,yy_test,cv=cv)
```




    array([0.92923786, 0.9321272 , 0.93476863, 0.93014162, 0.92625462])



### Random Forest

1. Hyper performance tuning


```python
from sklearn.model_selection import GridSearchCV

param_grid = {  'bootstrap': [True], 'max_depth': [5, 10, None], 'max_features': ['auto', 'log2'],
              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}

rfr = RandomForestRegressor(random_state = 1)

g_search = GridSearchCV(estimator = rfr, param_grid = param_grid, 

                          cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)
g_search.fit(xx_train, yy_train)

##print(g_search.best_params_)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:416: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.
      warn(





<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=1), n_jobs=1,
             param_grid={&#x27;bootstrap&#x27;: [True], &#x27;max_depth&#x27;: [5, 10, None],
                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;log2&#x27;],
                         &#x27;n_estimators&#x27;: [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]},
             return_train_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=1), n_jobs=1,
             param_grid={&#x27;bootstrap&#x27;: [True], &#x27;max_depth&#x27;: [5, 10, None],
                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;log2&#x27;],
                         &#x27;n_estimators&#x27;: [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]},
             return_train_score=True)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(random_state=1)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" ><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>




```python
random = RandomForestRegressor(n_estimators = 100, random_state = 0) 
```

2. Fitting


```python
random.fit(xx_train , yy_train)
```




<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestRegressor(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" checked><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestRegressor</label><div class="sk-toggleable__content"><pre>RandomForestRegressor(random_state=0)</pre></div></div></div></div></div>



3. Scoring


```python
print('Random_forest_score :',random.score(xx_test, yy_test))
random_score=random.score(xx_test, yy_test)
```

    Random_forest_score : 0.9614394304999585


#### Testing the Random Forest Model

1. Actual vs Predicted


```python
prediction = random.predict(xx_test)
```


```python
sns.regplot(yy_test,prediction)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
      warnings.warn(





    Text(0, 0.5, 'Predicted Y')




    
![png](output_210_2.png)
    


2. Performance metric of Random Forest Model


```python
from sklearn import metrics
print('MAE :'," ", metrics.mean_absolute_error(yy_test,prediction))
print('MSE :'," ", metrics.mean_squared_error(yy_test,prediction))
print('RMAE :'," ", np.sqrt(metrics.mean_squared_error(yy_test,prediction)))
```

    MAE :   1.0614610918888534
    MSE :   3.093761083634542
    RMAE :   1.7589090606493964



```python
random_score=random.score(xx_test, yy_test)
random_MAE=metrics.mean_absolute_error(yy_test,prediction)
random_MSE=metrics.mean_squared_error(yy_test,prediction)
random_RMAE=np.sqrt(metrics.mean_squared_error(yy_test,prediction))
```


```python
sns.distplot(yy_test - prediction,bins=50)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
      warnings.warn(msg, FutureWarning)





    <AxesSubplot:xlabel='price', ylabel='Density'>




    
![png](output_214_2.png)
    


3. K Cross-validation


```python
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
cross_val_score(RandomForestRegressor(),xx_test,yy_test,cv=cv)
```




    array([0.95894247, 0.96159066, 0.96204351, 0.96136511, 0.95847716])



### Gradient Boosting Regressor

1. Hyper performance tuning


```python
from sklearn import ensemble
clf = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 5)
```

2. Fitting


```python
clf.fit(xx_train, yy_train)
```




<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GradientBoostingRegressor(max_depth=5, n_estimators=400)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" checked><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">GradientBoostingRegressor</label><div class="sk-toggleable__content"><pre>GradientBoostingRegressor(max_depth=5, n_estimators=400)</pre></div></div></div></div></div>



3. Scoring


```python
print('Grdient_Boosting_Regressor_score :',clf.score(xx_test, yy_test))
```

    Grdient_Boosting_Regressor_score : 0.9677441515595245


#### Testing the Gradient Booster Model

1. Actual vs Predicted


```python
prediction=clf.predict(xx_test)
```


```python
plt.scatter(yy_test,prediction)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')
```




    Text(0, 0.5, 'Predicted Y')




    
![png](output_227_1.png)
    


2. Performace metric of Gradient Boosting Model


```python
from sklearn import metrics
print('MAE :'," ", metrics.mean_absolute_error(yy_test,prediction))
print('MSE :'," ", metrics.mean_squared_error(yy_test,prediction))
print('RMAE :'," ", np.sqrt(metrics.mean_squared_error(yy_test,prediction)))
```

    MAE :   1.0172474051201041
    MSE :   2.5879256950458056
    RMAE :   1.608703109664989



```python
Gradient_score=clf.score(xx_test, yy_test)
Gradient_MAE=metrics.mean_absolute_error(yy_test,prediction)
Gradient_MSE=metrics.mean_squared_error(yy_test,prediction)
Gradient_RMAE=np.sqrt(metrics.mean_squared_error(yy_test,prediction))
```


```python
sns.distplot(yy_test - prediction,bins=50)
```

    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
      warnings.warn(msg, FutureWarning)





    <AxesSubplot:xlabel='price', ylabel='Density'>




    
![png](output_231_2.png)
    


3. K Cross-validation


```python
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
cross_val_score(clf,xx_test,yy_test,cv=cv)
```




    array([0.96512808, 0.96756334, 0.96765565, 0.96697871, 0.96499137])



#### Price prediction function


```python
new_uber.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>distance</th>
      <th>surge_multiplier</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>temperature</th>
      <th>precipIntensity</th>
      <th>precipProbability</th>
      <th>temperatureLow</th>
      <th>cloudCover</th>
      <th>moonPhase</th>
      <th>precipIntensityMax</th>
      <th>month</th>
      <th>source</th>
      <th>destination</th>
      <th>cab_type</th>
      <th>product_id</th>
      <th>name</th>
      <th>uvIndex</th>
      <th>day-of-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>42.34</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>34.19</td>
      <td>0.72</td>
      <td>0.30</td>
      <td>0.1276</td>
      <td>12</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>8</td>
      <td>7</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>43.58</td>
      <td>0.1299</td>
      <td>1.0</td>
      <td>42.10</td>
      <td>1.00</td>
      <td>0.64</td>
      <td>0.1300</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>12</td>
      <td>2</td>
      <td>0</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>38.33</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>33.10</td>
      <td>0.03</td>
      <td>0.68</td>
      <td>0.1064</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>7</td>
      <td>5</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>34.38</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>28.90</td>
      <td>0.00</td>
      <td>0.75</td>
      <td>0.0000</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>10</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.44</td>
      <td>0</td>
      <td>42.2148</td>
      <td>-71.033</td>
      <td>37.44</td>
      <td>0.0000</td>
      <td>0.0</td>
      <td>36.71</td>
      <td>0.44</td>
      <td>0.72</td>
      <td>0.0001</td>
      <td>11</td>
      <td>5</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>6</td>
      <td>0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
def predict_price(name,source,surge_multiplier,icon):    
    loc_index = np.where(new_uber.columns==name)[0]

    x = np.zeros(len(new_uber.columns))
    x[0] = source
    x[1] = surge_multiplier
    x[2] = icon
    if loc_index >= 0:
        x[loc_index] = 1

    return random.predict([x])[0]
```


```python
pre= random.predict(xx_test)
```

####  Follow  these instructions before predicting the price:

-  **For cab_name**: Black SUV --> 0 , Lux --> 1 , Shared --> 2 , Taxi --> 3 , UberPool --> 4 , UberX --> 5


- **For Source**: Back Bay --> 0 , Beacon Hill --> 1 , Boston University --> 2 , Fenway --> 3 , Financial District --> 4 , Haymarket Square --> 5 , North End --> 6 , North Station --> 7 , Northeastern University --> 8 , South Station --> 9 , Theatre District --> 10 , West End --> 11


- **For Surge_multiplier** : Enter Surge Multiplier value from 0 to 4


- **for Icon**:  clear-day  --> 0 , clear-night  --> 1 , cloudy  --> 2 , fog  --> 3 , partly-cloudy-day  --> 4 , partly-cloudy-night  --> 5 , rain  --> 6


**predict_price(cab_name , source , surge_multiplier , icon)**


```python
predict_price(2 , 3, 2, 5)
```

    /var/folders/bx/1_m4xfh535j3lyq9x8njj6140000gn/T/ipykernel_57703/3659057063.py:8: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
      if loc_index >= 0:
    /Users/mohan/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names
      warnings.warn(





    25.808333333333334




# Result Metrics


```python
l1=['Linear Regression',linear_score]
l2=['Decission Tree',decision_score]
l3=['Random Forest',random_score]
l4=['Gradient_Boosting',Gradient_score]
cols=['Algo','Accuracy']
df= pd.DataFrame([list(l1),list(l2),list(l3),list(l4)],columns=cols)
df.set_index('Algo').plot(kind='bar')
plt.title("accuracy for each algorithm")
plt.ylabel("score*100")
plt.xlabel("algorithm")
```




    Text(0.5, 0, 'algorithm')




    
![png](output_242_1.png)
    



```python
l1=['Linear Regression',linear_MAE,linear_MSE,linear_RMAE]
l2=['Decission Tree',decision_MAE,decision_MSE,decision_RMAE]
l3=['Random Forest',random_MAE,random_MSE,random_RMAE]
l4=['Gradient_Boosting',Gradient_MAE,Gradient_MSE,Gradient_RMAE]
cols=['Algo','MAE','MSE','RMAE']
```


```python
df= pd.DataFrame([list(l1),list(l2),list(l3),list(l4)],columns=cols)

```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Algo</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regression</td>
      <td>4.786488</td>
      <td>38.453075</td>
      <td>6.201054</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Decission Tree</td>
      <td>1.017236</td>
      <td>2.787818</td>
      <td>1.669676</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Random Forest</td>
      <td>1.061461</td>
      <td>3.093761</td>
      <td>1.758909</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Gradient_Boosting</td>
      <td>1.017247</td>
      <td>2.587926</td>
      <td>1.608703</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.set_index('Algo').plot(kind='bar')
plt.title("performance matrix for each algorithm")
plt.xlabel("performance matrix")
plt.ylabel("algorithm")
```




    Text(0, 0.5, 'algorithm')




    
![png](output_246_1.png)
    


The attributes on the left side that is red color coded has more impact on the price than the blue portion. That is, surge multiplier, product ID, Name and distance.


```python
import shap
explainer=shap.TreeExplainer(decision)
shap_values = explainer.shap_values(X_test)
shap_plot = shap.force_plot(explainer.expected_value, 
    shap_values[-1:], features=X_test.iloc[-1:], 
    feature_names=X_test.columns[0:20],
    matplotlib=True, show=False, plot_cmap=['#77dd77', '#f99191'])
```


    ---------------------------------------------------------------------------

    ModuleNotFoundError                       Traceback (most recent call last)

    /Users/mohan/Downloads/updated_data/Uber and Lyft price prediction -INFO7390.ipynb Cell 249 in <cell line: 1>()
    ----> <a href='vscode-notebook-cell:/Users/mohan/Downloads/updated_data/Uber%20and%20Lyft%20price%20prediction%20-INFO7390.ipynb#Y445sZmlsZQ%3D%3D?line=0'>1</a> import shap
          <a href='vscode-notebook-cell:/Users/mohan/Downloads/updated_data/Uber%20and%20Lyft%20price%20prediction%20-INFO7390.ipynb#Y445sZmlsZQ%3D%3D?line=1'>2</a> explainer=shap.TreeExplainer(decision)
          <a href='vscode-notebook-cell:/Users/mohan/Downloads/updated_data/Uber%20and%20Lyft%20price%20prediction%20-INFO7390.ipynb#Y445sZmlsZQ%3D%3D?line=2'>3</a> shap_values = explainer.shap_values(X_test)


    ModuleNotFoundError: No module named 'shap'

